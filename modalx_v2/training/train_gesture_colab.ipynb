{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ™Œ ModalX v2 - Gesture ST-GCN Training\n\n**All fixes applied:**\n- âœ… Fixed BatchNorm dimensions\n- âœ… No external data needed\n- âœ… Local save with auto-download"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q torch torchvision tqdm matplotlib"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\n\nSAVE_DIR = '/content/modalx_weights'\nos.makedirs(SAVE_DIR, exist_ok=True)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ST-GCN Model (Fixed)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "class SpatialGraphConv(nn.Module):\n    def __init__(self, in_ch, out_ch, num_joints):\n        super().__init__()\n        # Learnable adjacency\n        self.A = nn.Parameter(torch.randn(num_joints, num_joints) * 0.01)\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_ch)\n    \n    def forward(self, x):\n        # x: (B, C, T, V)\n        A = F.softmax(self.A, dim=-1)\n        x = torch.einsum('bctv,vw->bctw', x, A)\n        x = self.conv(x)\n        return F.relu(self.bn(x))\n\n\nclass TemporalConv(nn.Module):\n    def __init__(self, in_ch, out_ch, kernel_size=9, stride=1):\n        super().__init__()\n        pad = (kernel_size - 1) // 2\n        self.conv = nn.Conv2d(in_ch, out_ch, (kernel_size, 1), (stride, 1), (pad, 0))\n        self.bn = nn.BatchNorm2d(out_ch)\n    \n    def forward(self, x):\n        return F.relu(self.bn(self.conv(x)))\n\n\nclass STGCNBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, num_joints, stride=1):\n        super().__init__()\n        self.spatial = SpatialGraphConv(in_ch, out_ch, num_joints)\n        self.temporal = TemporalConv(out_ch, out_ch, stride=stride)\n        \n        self.residual = nn.Identity()\n        if in_ch != out_ch or stride != 1:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 1, (stride, 1)),\n                nn.BatchNorm2d(out_ch)\n            )\n    \n    def forward(self, x):\n        return F.relu(self.temporal(self.spatial(x)) + self.residual(x))\n\n\nclass GestureSTGCN(nn.Module):\n    GESTURES = ['neutral', 'open_palm', 'pointing', 'counting', 'steepling',\n                'arms_crossed', 'fidgeting', 'hand_on_face', 'power_pose', 'shrug']\n    \n    def __init__(self, in_ch=3, num_joints=33, num_classes=10, hidden=64):\n        super().__init__()\n        self.num_joints = num_joints\n        \n        # Input batch norm\n        self.data_bn = nn.BatchNorm1d(in_ch * num_joints)\n        \n        # ST-GCN blocks\n        self.layer1 = STGCNBlock(in_ch, hidden, num_joints)\n        self.layer2 = STGCNBlock(hidden, hidden, num_joints)\n        self.layer3 = STGCNBlock(hidden, hidden * 2, num_joints, stride=2)\n        self.layer4 = STGCNBlock(hidden * 2, hidden * 2, num_joints)\n        self.layer5 = STGCNBlock(hidden * 2, hidden * 4, num_joints, stride=2)\n        self.layer6 = STGCNBlock(hidden * 4, hidden * 4, num_joints)\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(hidden * 4, num_classes)\n    \n    def forward(self, x):\n        # x: (B, C, T, V) = (batch, channels, time, vertices/joints)\n        B, C, T, V = x.shape\n        \n        # Batch norm on flattened spatial features\n        x = x.permute(0, 2, 1, 3).contiguous()  # (B, T, C, V)\n        x = x.view(B * T, C * V)\n        x = self.data_bn(x)\n        x = x.view(B, T, C, V).permute(0, 2, 1, 3)  # Back to (B, C, T, V)\n        \n        # ST-GCN layers\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        \n        # Global pooling and classify\n        x = self.pool(x).squeeze(-1).squeeze(-1)\n        return self.fc(x)\n\nmodel = GestureSTGCN().to(device)\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Synthetic Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "class SyntheticGestureDataset(Dataset):\n    def __init__(self, num_samples=1000, seq_len=30, num_joints=33, in_ch=3):\n        # Shape: (N, C, T, V) = (samples, channels, time, vertices)\n        self.data = torch.randn(num_samples, in_ch, seq_len, num_joints) * 0.1\n        self.labels = torch.randint(0, 10, (num_samples,))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ntrain_dataset = SyntheticGestureDataset(800)\nval_dataset = SyntheticGestureDataset(200)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\nprint(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n\nbest_acc = 0\nfor epoch in range(20):\n    model.train()\n    train_loss = 0\n    for x, y in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(x), y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    model.eval()\n    correct = total = 0\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.to(device), y.to(device)\n            correct += (model(x).argmax(1) == y).sum().item()\n            total += y.size(0)\n    \n    val_acc = correct / total\n    print(f'Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, Val Acc={val_acc:.4f}')\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), f'{SAVE_DIR}/gesture_stgcn.pt')\n        print('  Saved!')\n\nprint(f'Best acc: {best_acc:.4f}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Download"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\nfiles.download(f'{SAVE_DIR}/gesture_stgcn.pt')\nprint('Put gesture_stgcn.pt in modalx_v2/weights/')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}