{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": ["# ðŸŽ­ ModalX v2 - Emotion Model Training\n\n**All fixes applied:**\n- âœ… No Kaggle needed (synthetic data)\n- âœ… No Google Drive needed (local save)\n- âœ… Auto-download after training\n\n---"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["# Install dependencies\n!pip install -q torch torchaudio scikit-learn tqdm matplotlib"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')\n\n# Create save directory\nSAVE_DIR = '/content/modalx_weights'\nos.makedirs(SAVE_DIR, exist_ok=True)\nprint(f'Weights will be saved to: {SAVE_DIR}')"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Dataset (Synthetic for testing)\nâš ï¸ Replace with real RAVDESS/TESS for competition!"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["class EmotionDataset(Dataset):\n    EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise', 'calm']\n    \n    def __init__(self, num_samples=1000, sample_rate=16000, duration=3.0):\n        self.num_samples = num_samples\n        self.audio_length = int(sample_rate * duration)\n        self.labels = torch.randint(0, 8, (num_samples,))\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        waveform = torch.randn(self.audio_length) * 0.1\n        return waveform, self.labels[idx].item()\n\ntrain_dataset = EmotionDataset(1600)\nval_dataset = EmotionDataset(400)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n\nprint(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Model Definition"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["class AttentionPooling(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.Tanh(),\n            nn.Linear(embed_dim // 2, 1)\n        )\n    \n    def forward(self, x):\n        weights = F.softmax(self.attention(x), dim=1)\n        return torch.sum(x * weights, dim=1)\n\n\nclass TransformerEmotionModel(nn.Module):\n    EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise', 'calm']\n    \n    def __init__(self, n_mels=80, embed_dim=256, num_heads=4, num_layers=3, num_emotions=8, dropout=0.1, sample_rate=16000):\n        super().__init__()\n        \n        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate, n_fft=400, hop_length=160, n_mels=n_mels\n        )\n        \n        self.cnn_extractor = nn.Sequential(\n            nn.Conv1d(n_mels, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2),\n            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm1d(256), nn.ReLU(), nn.MaxPool1d(2),\n            nn.Conv1d(256, embed_dim, kernel_size=3, padding=1),\n            nn.BatchNorm1d(embed_dim), nn.ReLU(), nn.MaxPool1d(2),\n        )\n        \n        self.pos_encoding = nn.Parameter(torch.randn(1, 1000, embed_dim) * 0.02)\n        \n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=embed_dim, nhead=num_heads,\n                dim_feedforward=embed_dim * 4, dropout=dropout, batch_first=True\n            )\n            for _ in range(num_layers)\n        ])\n        \n        self.attention_pool = AttentionPooling(embed_dim)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(embed_dim // 2, num_emotions)\n        )\n    \n    def forward(self, waveform):\n        mel = self.mel_spec(waveform)\n        mel = torch.log(mel + 1e-9)\n        features = self.cnn_extractor(mel)\n        features = features.transpose(1, 2)\n        seq_len = features.size(1)\n        features = features + self.pos_encoding[:, :seq_len, :]\n        for layer in self.transformer_layers:\n            features = layer(features)\n        pooled = self.attention_pool(features)\n        return self.classifier(pooled)\n\nmodel = TransformerEmotionModel().to(device)\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Training"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\nnum_epochs = 20\nbest_val_acc = 0\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_correct, train_total = 0, 0, 0\n    \n    for waveforms, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n        waveforms, labels = waveforms.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(waveforms)\n        loss = criterion(logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        train_loss += loss.item()\n        train_correct += (logits.argmax(-1) == labels).sum().item()\n        train_total += labels.size(0)\n    \n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for waveforms, labels in val_loader:\n            waveforms, labels = waveforms.to(device), labels.to(device)\n            logits = model(waveforms)\n            loss = criterion(logits, labels)\n            val_loss += loss.item()\n            val_correct += (logits.argmax(-1) == labels).sum().item()\n            val_total += labels.size(0)\n    \n    scheduler.step()\n    train_acc = train_correct / train_total\n    val_acc = val_correct / val_total\n    history['train_loss'].append(train_loss / len(train_loader))\n    history['val_loss'].append(val_loss / len(val_loader))\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n    \n    print(f'Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), f'{SAVE_DIR}/emotion_transformer.pt')\n        print(f'  Saved best model!')\n\nprint(f'Best val acc: {best_val_acc:.4f}')"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Visualize Results"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["fig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Val')\naxes[0].set_title('Loss')\naxes[0].legend()\naxes[1].plot(history['train_acc'], label='Train')\naxes[1].plot(history['val_acc'], label='Val')\naxes[1].set_title('Accuracy')\naxes[1].legend()\nplt.tight_layout()\nplt.show()"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Download Model"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": ["from google.colab import files\nfiles.download(f'{SAVE_DIR}/emotion_transformer.pt')\nprint('Done! Put emotion_transformer.pt in modalx_v2/weights/')"],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
