{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üìù ModalX v2 - Content BERT Training\n\n**Model:** Fine-tuned DistilBERT for Content Quality\n\n**Task:** Multi-task learning for argument, vocabulary, structure scoring"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q torch transformers datasets scikit-learn tqdm"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\ndrive.mount('/content/drive')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "class ContentBERT(nn.Module):\n    def __init__(self, hidden=768, dropout=0.3):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.pooler = nn.Sequential(nn.Linear(hidden, hidden), nn.Tanh())\n        \n        self.argument_head = nn.Sequential(\n            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(256, 1), nn.Sigmoid()\n        )\n        self.vocab_head = nn.Sequential(\n            nn.Linear(hidden, 128), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(128, 3)\n        )\n        self.structure_head = nn.Sequential(\n            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(256, 1), nn.Sigmoid()\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls = self.pooler(out.last_hidden_state[:, 0, :])\n        return {\n            'argument': self.argument_head(cls),\n            'vocab': self.vocab_head(cls),\n            'structure': self.structure_head(cls)\n        }\n\nmodel = ContentBERT().to(device)\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Sample training data\nSAMPLES = [\n    ('Therefore, our methodology demonstrates significant impact.', 0.9, 2, 0.8),\n    ('Um, so like, we did some stuff.', 0.2, 0, 0.3),\n    ('The results indicate a 40% improvement in efficiency.', 0.85, 2, 0.9),\n    ('It was good I think maybe.', 0.1, 0, 0.2),\n]\n\nclass ContentDataset(Dataset):\n    def __init__(self, samples, tokenizer):\n        self.samples = samples\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        text, arg, vocab, struct = self.samples[idx]\n        enc = self.tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(),\n            'attention_mask': enc['attention_mask'].squeeze(),\n            'argument': torch.tensor(arg),\n            'vocab': torch.tensor(vocab),\n            'structure': torch.tensor(struct)\n        }\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndataset = ContentDataset(SAMPLES * 50, tokenizer)\nloader = DataLoader(dataset, batch_size=8, shuffle=True)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nmse = nn.MSELoss()\nce = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(loader):\n        optimizer.zero_grad()\n        out = model(\n            batch['input_ids'].to(device),\n            batch['attention_mask'].to(device)\n        )\n        loss = mse(out['argument'].squeeze(), batch['argument'].float().to(device))\n        loss += ce(out['vocab'], batch['vocab'].to(device))\n        loss += mse(out['structure'].squeeze(), batch['structure'].float().to(device))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}')\n\ntorch.save(model.state_dict(), '/content/drive/MyDrive/modalx_v2/content_bert.pt')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}